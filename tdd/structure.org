Plot behind the unit testing talk

* TODO find some other talks about unit testing and take some material

* TODO add an example of how to debug a small program that has tests and one that doesn't have tests.
  and how writing tests can also drive you finding a possible bug.

* Why need testing

** No help from the compiler

** Very bad errors coming only at runtime

   examples of functions that fail miserably only at runtime

** We need to test our code even harder

* Functions

** Pure functions

** Side effects, and why they are bad

** Test pure functions (with a simple module)

* Testing

** Unit testing, integration testing, functional testing

** TDD and test first development

** Examples and demo about TDD

** Regression testing

** Remove the fear of touching code

** Testing as documentation, show how to use

* unittest libraries

** Setup / Teardown

** Test discovery

** Coverage

** Nose + coverage

* Mock library

** Python dynamic binding and by-hand examples

** Testing side effect functions

* Conclusions

** Avoid side effect whenever possible

** Build your programs bottom-up, composing small functions

** Before writing the code, think how you could test it

* What to say

Tesitng

Test-driven development cycle

A graphical representation of the development cycle, using a basic flowchart
The following sequence is based on the book Test-Driven Development by Example.[1]

[edit]Add a test In test-driven development, each new feature begins
with writing a test. This test must inevitably fail because it is
written before the feature has been implemented. (If it does not fail,
then either the proposed "new" feature already exists or the test is
defective.) To write a test, the developer must clearly understand the
feature's specification and requirements. The developer can accomplish
this through use cases and user stories to cover the requirements and
exception conditions. This could also imply a variant, or modification
of an existing test. This is a differentiating feature of test-driven
development versus writing unit tests after the code is written: it
makes the developer focus on the requirements before writing the code,
a subtle but important difference.

[edit]Run all tests and see if the new one fails This validates that
the test harness is working correctly and that the new test does not
mistakenly pass without requiring any new code. This step also tests
the test itself, in the negative: it rules out the possibility that
the new test will always pass, and therefore be worthless. The new
test should also fail for the expected reason. This increases
confidence (although it does not entirely guarantee) that it is
testing the right thing, and will pass only in intended cases.

[edit]Write some code The next step is to write some code that will
cause the test to pass. The new code written at this stage will not be
perfect and may, for example, pass the test in an inelegant way. That
is acceptable because later steps will improve and hone it.

This additional step in test-first programming is the main difference
between it and invariant-based programming.

It is important that the code written is only designed to pass the
test; no further (and therefore untested) functionality should be
predicted and 'allowed for' at any stage.

[edit]Run the automated tests and see them succeed If all test cases
now pass, the programmer can be confident that the code meets all the
tested requirements. This is a good point from which to begin the
final step of the cycle.

[edit]Refactor code Now the code can be cleaned up as necessary. By
re-running the test cases, the developer can be confident that code
refactoring is not damaging any existing functionality. The concept of
removing duplication is an important aspect of any software design. In
this case, however, it also applies to removing any duplication
between the test code and the production code — for example magic
numbers or strings that were repeated in both, in order to make the
test pass in step 3.

[edit]Repeat Starting with another new test, the cycle is then
repeated to push forward the functionality. The size of the steps
should always be small, with as few as 1 to 10 edits between each test
run. If new code does not rapidly satisfy a new test, or other tests
fail unexpectedly, the programmer should undo or revert in preference
to excessive debugging. Continuous integration helps by providing
revertible checkpoints. When using external libraries it is important
not to make increments that are so small as to be effectively merely
testing the library itself,[3] unless there is some reason to believe
that the library is buggy or is not sufficiently feature-complete to
serve all the needs of the main program being written.

[edit]Development style

There are various aspects to using test-driven development, for
example the principles of "keep it simple stupid" (KISS) and "You
aren't gonna need it" (YAGNI). By focusing on writing only the code
necessary to pass tests, designs can be cleaner and clearer than is
often achieved by other methods.[1] In Test-Driven Development by
Example, Kent Beck also suggests the principle "Fake it till you make
it".

To achieve some advanced design concept (such as a design pattern),
tests are written that will generate that design. The code may remain
simpler than the target pattern, but still pass all required
tests. This can be unsettling at first but it allows the developer to
focus only on what is important.

Write the tests first. The tests should be written before the
functionality that is being tested. This has been claimed to have many
benefits. It helps ensure that the application is written for
testability, as the developers must consider how to test the
application from the outset, rather than worrying about it later. It
also ensures that tests for every feature will be
written. Additionally, writing the tests first drives a deeper and
earlier understanding of the product requirements, ensures the
effectiveness of the test code, and maintains a continual focus on the
quality of the product.[6] When writing feature-first code, there is a
tendency by developers and the development organisations to push the
developer on to the next feature, neglecting testing entirely. The
first test might not even compile, at first, because all of the
classes and methods it requires may not yet exist. Nevertheless, that
first test functions as an executable specification.[7]

First fail the test cases. The idea is to ensure that the test really
works and can catch an error. Once this is shown, the underlying
functionality can be implemented. This has been coined the
"test-driven development mantra", known as red/green/refactor where
red means fail and green is pass.

Test-driven development constantly repeats the steps of adding test
cases that fail, passing them, and refactoring. Receiving the expected
test results at each stage reinforces the programmer's mental model of
the code, boosts confidence and increases productivity.

Keep the unit small. For TDD, a unit is most commonly defined as a
class or group of related functions, often called a module. Keeping
units relatively small is claimed to provide critical benefits,
including:

Reduced Debugging Effort – When test failures are detected, having
smaller units aids in tracking down errors.  Self-Documenting Tests –
Small test cases have improved readability and facilitate rapid
understandability.[6] Advanced practices of test-driven development
can lead to Acceptance Test-driven development (ATDD) where the
criteria specified by the customer are automated into acceptance
tests, which then drive the traditional unit test-driven development
(UTDD) process.[8] This process ensures the customer has an automated
mechanism to decide whether the software meets their
requirements. With ATDD, the development team now has a specific
target to satisfy, the acceptance tests, which keeps them continuously
focused on what the customer really wants from that user story.

* Benefits
A 2005 study found that using TDD meant writing more tests and, in
turn, programmers who wrote more tests tended to be more
productive.[10] Hypotheses relating to code quality and a more direct
correlation between TDD and productivity were inconclusive.[11]

Programmers using pure TDD on new ("greenfield") projects reported
they only rarely felt the need to invoke a debugger. Used in
conjunction with a version control system, when tests fail
unexpectedly, reverting the code to the last version that passed all
tests may often be more productive than debugging.[12]

Test-driven development offers more than just simple validation of
correctness, but can also drive the design of a program.[citation
needed] By focusing on the test cases first, one must imagine how the
functionality will be used by clients (in the first case, the test
cases). So, the programmer is concerned with the interface before the
implementation. This benefit is complementary to Design by Contract as
it approaches code through test cases rather than through mathematical
assertions or preconceptions.

Test-driven development offers the ability to take small steps when
required. It allows a programmer to focus on the task at hand as the
first goal is to make the test pass. Exceptional cases and error
handling are not considered initially, and tests to create these
extraneous circumstances are implemented separately. Test-driven
development ensures in this way that all written code is covered by at
least one test. This gives the programming team, and subsequent users,
a greater level of confidence in the code.

While it is true that more code is required with TDD than without TDD
because of the unit test code, total code implementation time is
typically shorter.[13] Large numbers of tests help to limit the number
of defects in the code. The early and frequent nature of the testing
helps to catch defects early in the development cycle, preventing them
from becoming endemic and expensive problems. Eliminating defects
early in the process usually avoids lengthy and tedious debugging
later in the project.

TDD can lead to more modularized, flexible, and extensible code. This
effect often comes about because the methodology requires that the
developers think of the software in terms of small units that can be
written and tested independently and integrated together later. This
leads to smaller, more focused classes, looser coupling, and cleaner
interfaces. The use of the mock object design pattern also contributes
to the overall modularization of the code because this pattern
requires that the code be written so that modules can be switched
easily between mock versions for unit testing and "real" versions for
deployment.

Because no more code is written than necessary to pass a failing test
case, automated tests tend to cover every code path. For example, in
order for a TDD developer to add an else branch to an existing if
statement, the developer would first have to write a failing test case
that motivates the branch. As a result, the automated tests resulting
from TDD tend to be very thorough: they will detect any unexpected
changes in the code's behaviour. This detects problems that can arise
where a change later in the development cycle unexpectedly alters
other functionality.


* Structure
  - why it's important, talking specifically how Python behaves,
    show examples of great failures

  - what is unit testing
  - what is TDD, and how it work
